import pandas as pd
from pathlib import Path
from PIL import Image
import numpy as np

def validate_image_for_ml(img_path, min_size=50, max_size=10000):
    """
    V√©rifie si une image est viable pour le ML
    
    Args:
        img_path: Chemin vers l'image
        min_size: Taille minimale (largeur/hauteur)
        max_size: Taille maximale (largeur/hauteur)
    
    Returns:
        (bool, str): (est_valide, message_erreur)
    """
    try:
        # 1. Ouvre et v√©rifie l'int√©grit√© basique
        with Image.open(img_path) as img:
            # V√©rifie que c'est bien une image
            img.verify()
        
        # 2. Recharge l'image (verify() ferme le fichier)
        with Image.open(img_path) as img:
            # V√©rifie le format
            if img.format not in ['JPEG', 'JPG', 'PNG', 'BMP', 'TIFF']:
                return False, f"Format non support√©: {img.format}"
            
            # V√©rifie les dimensions
            width, height = img.size
            if width < min_size or height < min_size:
                return False, f"Trop petite: {width}x{height}"
            
            if width > max_size or height > max_size:
                return False, f"Trop grande: {width}x{height}"
            
            # V√©rifie que l'image peut √™tre convertie en RGB
            try:
                img_rgb = img.convert('RGB')
            except Exception as e:
                return False, f"Conversion RGB impossible: {e}"
            
            # 3. V√©rifie que l'image a du contenu (pas juste du blanc/noir)
            img_array = np.array(img_rgb)
            
            # Check que ce n'est pas une image vide
            if img_array.size == 0:
                return False, "Image vide (0 pixels)"
            
            # V√©rifie la variance (image pas compl√®tement uniforme)
            variance = np.var(img_array)
            if variance < 1.0:  # Variance tr√®s faible = image uniforme
                return False, f"Variance trop faible: {variance:.2f}"
            
            # 4. Test de redimensionnement (simulation transform ML)
            try:
                img_resized = img_rgb.resize((224, 224), Image.LANCZOS)
                # V√©rifie que le resize n'a pas produit une image noire
                resized_array = np.array(img_resized)
                if np.mean(resized_array) < 1.0:
                    return False, "Image noire apr√®s resize"
            except Exception as e:
                return False, f"Resize impossible: {e}"
            
            # 5. V√©rifie la taille du fichier
            file_size = img_path.stat().st_size
            if file_size < 1000:  # Moins de 1KB
                return False, f"Fichier trop petit: {file_size} bytes"
            
            return True, "OK"
            
    except Exception as e:
        return False, f"Erreur ouverture: {str(e)[:50]}"


def create_csv_with_validation():
    """
    Cr√©e un CSV avec validation compl√®te des images
    """
    print("üìä CR√âATION DU CSV AVEC VALIDATION DES IMAGES")
    print("="*70)
    
    hf_dir = Path("API/ML/data/hugging-face")
    all_rows = []
    
    stats = {
        'total_scanned': 0,
        'valid': 0,
        'corrupted': 0,
        'errors': {}
    }
    
    for split_name in ['train', 'validation', 'test']:
        split_dir = hf_dir / split_name
        
        if not split_dir.exists():
            print(f"‚ö†Ô∏è  {split_dir} non trouv√©")
            continue
        
        print(f"\nüìÇ Processing {split_name}...")
        print("-" * 70)
        
        for class_folder in ['Anemia', 'NoAnemia']:
            class_dir = split_dir / class_folder
            
            if not class_dir.exists():
                print(f"   ‚ö†Ô∏è  {class_dir} non trouv√©")
                continue
            
            anemia_label = 1 if class_folder == 'Anemia' else 0
            
            # Liste toutes les images
            image_files = sorted(list(class_dir.glob('*.jpg')) + 
                               list(class_dir.glob('*.png')) + 
                               list(class_dir.glob('*.jpeg')))
            
            print(f"\n   {class_folder}: {len(image_files)} images √† v√©rifier")
            
            valid_count = 0
            
            for idx, img_path in enumerate(image_files):
                stats['total_scanned'] += 1
                
                # Validation compl√®te
                is_valid, error_msg = validate_image_for_ml(img_path)
                
                if is_valid:
                    # Image OK, ajoute au CSV
                    relative_path = f"hugging-face/{split_name}/{class_folder}/{img_path.name}"
                    
                    all_rows.append({
                        'patient_number': f'HF_{split_name}_{class_folder}_{valid_count}',
                        'image_path': relative_path,
                        'image_name': img_path.name,
                        'anemia_label': anemia_label,
                        'hgb': -1.0,
                        'gender': 'U',
                        'age': -1,
                        'n_images': 1,
                        'source': 'huggingface',
                        'split': split_name,
                        'class': class_folder
                    })
                    
                    valid_count += 1
                    stats['valid'] += 1
                else:
                    # Image corrompue, log l'erreur
                    print(f"      ‚ùå {img_path.name}: {error_msg}")
                    stats['corrupted'] += 1
                    
                    # Compte les types d'erreurs
                    error_type = error_msg.split(':')[0]
                    stats['errors'][error_type] = stats['errors'].get(error_type, 0) + 1
            
            print(f"      ‚úÖ {valid_count} images valides")
            if len(image_files) - valid_count > 0:
                print(f"      ‚ùå {len(image_files) - valid_count} images rejet√©es")
    
    # Cr√©e le DataFrame
    df = pd.DataFrame(all_rows)
    
    print(f"\n" + "="*70)
    print(f"üìä STATISTIQUES DE VALIDATION:")
    print(f"="*70)
    print(f"\nTotal scann√©: {stats['total_scanned']} images")
    print(f"‚úÖ Valides: {stats['valid']} ({stats['valid']/stats['total_scanned']*100:.1f}%)")
    print(f"‚ùå Corrompues: {stats['corrupted']} ({stats['corrupted']/stats['total_scanned']*100:.1f}%)")
    
    if stats['errors']:
        print(f"\nüìã Types d'erreurs rencontr√©es:")
        for error_type, count in sorted(stats['errors'].items(), key=lambda x: x[1], reverse=True):
            print(f"   - {error_type}: {count}")
    
    print(f"\n" + "="*70)
    print(f"üìä R√âSUM√â DATASET FINAL:")
    print(f"="*70)
    print(f"\nTotal images valides: {len(df)}")
    
    print(f"\nüìà Par split:")
    for split in ['train', 'validation', 'test']:
        count = (df['split'] == split).sum()
        if count > 0:
            print(f"   {split}: {count} images")
    
    print(f"\nüìä Distribution globale:")
    print(f"   An√©mie (1): {(df['anemia_label']==1).sum()} images")
    print(f"   Non-an√©mie (0): {(df['anemia_label']==0).sum()} images")
    
    print(f"\nüìä Distribution par split:")
    for split in ['train', 'validation', 'test']:
        df_split = df[df['split'] == split]
        if len(df_split) > 0:
            anem = (df_split['anemia_label']==1).sum()
            no_anem = (df_split['anemia_label']==0).sum()
            print(f"   {split}: An√©mie={anem}, Non-an√©mie={no_anem}")
    
    # Sauvegarde le CSV principal
    csv_path = hf_dir / 'anemia_eyes_all.csv'
    df.to_csv(csv_path, index=False)
    print(f"\nüíæ CSV sauvegard√©: {csv_path}")
    
    # Sauvegarde aussi un log des images corrompues
    if stats['corrupted'] > 0:
        corrupted_log = hf_dir / 'corrupted_images.txt'
        with open(corrupted_log, 'w') as f:
            f.write(f"Images corrompues trouv√©es: {stats['corrupted']}\n")
            f.write(f"Date: {pd.Timestamp.now()}\n\n")
            for error_type, count in stats['errors'].items():
                f.write(f"{error_type}: {count}\n")
        print(f"üìù Log des erreurs: {corrupted_log}")
    
    return df, stats


def verify_csv_images(csv_path):
    """
    V√©rifie que toutes les images du CSV sont accessibles et valides
    """
    print(f"\nüîç V√âRIFICATION FINALE DU CSV: {csv_path}")
    print("="*70)
    
    df = pd.read_csv(csv_path)
    base_dir = Path("API/ML/data")
    
    invalid_count = 0
    
    for idx, row in df.iterrows():
        img_path = base_dir / row['image_path']
        
        if not img_path.exists():
            print(f"‚ùå Fichier manquant: {row['image_path']}")
            invalid_count += 1
        else:
            is_valid, error_msg = validate_image_for_ml(img_path)
            if not is_valid:
                print(f"‚ùå Image invalide: {row['image_path']} - {error_msg}")
                invalid_count += 1
    
    if invalid_count == 0:
        print(f"‚úÖ Toutes les {len(df)} images du CSV sont valides et accessibles!")
    else:
        print(f"\n‚ö†Ô∏è  {invalid_count} images invalides trouv√©es sur {len(df)}")
    
    return invalid_count == 0


if __name__ == '__main__':
    # Cr√©e le CSV avec validation
    df, stats = create_csv_with_validation()
    
    # V√©rification finale
    if len(df) > 0:
        print("\n" + "="*70)
        verify_csv_images('API/ML/data/hugging-face/anemia_eyes_all.csv')
